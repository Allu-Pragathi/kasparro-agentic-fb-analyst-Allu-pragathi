# Minimal LLM client placeholder.
# In a production repo we'd implement OpenAI/other provider client here.
def call_llm(prompt, max_tokens=512):
    # placeholder deterministic stub for offline usage
    return "LLM_RESPONSE_PLACEHOLDER"
